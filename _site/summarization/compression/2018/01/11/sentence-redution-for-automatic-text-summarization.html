<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>Sentence Reduction for Automatic Text Summarization</title>
		<link rel="stylesheet" href="/css/style.css">
	</head>

	<body>
<!--
		<header>

			<div class="container">
				<nav class="main-nav">
					<ul>
						
						
							
						
							
						
							
								<li>
									<a href="/" >
										Home
									</a>
								</li>
							
						
							
								<li>
									<a href="/blog.html" >
										Blog
									</a>
								</li>
							
						
					</ul>
				</nav>
				<h1><a href="/">&nbsp;<strong></strong></a></h1>
			</div>

		</header>-->

		<div class="content">
			<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Sentence Reduction for Automatic Text Summarization</h1>
    <p class="post-meta">
      <time datetime="2018-01-11T12:40:53-05:00" itemprop="datePublished">
        
        Jan 11, 2018
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <ul>
  <li>Title: Sentence Reduction for Automatic Text Summarization <a href="http://delivery.acm.org/10.1145/980000/974190/p310-jing.pdf?ip=74.105.10.86&amp;id=974190&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=850146679&amp;CFTOKEN=43433700&amp;__acm__=1515685122_a0b141d0232617369741d8940165acb6">PDF</a></li>
  <li>Authors: Hongyan Jing</li>
  <li>Venue: Applied Natural Language Processing Conference (2000)</li>
</ul>

<h3 id="nutshell">Nutshell</h3>

<p>This is a very early sentence compression paper from Hongyan Jing, a student of <a href="http://www.cs.columbia.edu/~kathy/">Kathleen McKeown</a>. It was really interesting for me to read because I’ve looked at a bunch of recent work on the same topic.</p>

<p>To shorten a sentence, Jing does the following:</p>

<ul>
  <li>assigns an <strong>importance score</strong> using WordNet to each phrase in the sentence.</li>
  <li>uses a supervised corpus of summary–document pairs to determine the probability of pruning a constituent subtree of a given type, conditioned on its parent node.</li>
  <li>uses a constituent parser and lexical resource to identify mandatory grammatical components, guided by manual rules.</li>
  <li>traverses the constituent parse tree from top to bottom, pruning subtrees which (1) are not grammaticality necessary (2) have a high probably of removal by a human annotator and (3) have low importance.</li>
</ul>

<p>Jing evaluates her method using a custom metric, the ‘‘success rate’’, which measures the degree of overlap between a computer and a human annotator.</p>

<h3 id="discussion">Discussion</h3>

<p>One thing which is striking is the deep similarity between this work and later approaches to sentence compression. The core concerns in Jing (2000), <a href="http://www.aclweb.org/anthology/W08-1105">Filipova and Strube</a> (2008) and <a href="http://www.jair.org/papers/paper2433.html">Clarke and Lapata</a> (2010) are quite similar: the goal is to retain ‘‘important’’ portions of a source sentence while retaining grammaticality. The techniques for determining how to automatically meet these criteria have shifted, but in many ways the basic ideas are the same. Of course, <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43852.pdf">recent</a> sentence compression <a href="/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html">papers</a> use neural networks to try and replicate these aims in a “data-driven” fashion.</p>

<p>Based on 20 years of research, it seems like the sentence compression agenda in NLP might be <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41393.pdf">more data</a> + better modeling = closer automated replication of human summarization decisions. (As measured by ROUGE score or Pyramid or some other metric).</p>

<p>This paradigm is common across NLP. But for summarization, the approach often annoys me: it seems obvious that different kinds of users will have different information needs. Query-focused summarization is supposed to address this issue, but I’m sort of dubious that there even exists a “gold standard”, <em>perfect</em> summary for a given query. There seem to be deep limitations to this “annotate and model” paradigm, at least for sentence compression.</p>

  </div>

  
</article>

		</div>
		<footer>
			<div class="container">
				<p class="center-text"> <a href=""></a></p>
			</div>
		</footer>
		<script type="text/javascript"
			src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	</body>
</html>
