<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-28T12:26:36-05:00</updated><id>http://localhost:4000/</id><title type="html">Abe Handler</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Grammaticality, Acceptability and Probability</title><link href="http://localhost:4000/acceptability/linguistics/2018/01/26/grammaticality_acceptability_probability.html" rel="alternate" type="text/html" title="Grammaticality, Acceptability and Probability" /><published>2018-01-26T12:40:53-05:00</published><updated>2018-01-26T12:40:53-05:00</updated><id>http://localhost:4000/acceptability/linguistics/2018/01/26/grammaticality_acceptability_probability</id><content type="html" xml:base="http://localhost:4000/acceptability/linguistics/2018/01/26/grammaticality_acceptability_probability.html">&lt;ul&gt;
  &lt;li&gt;Title: Grammaticality, Acceptability and Probability &lt;a href=&quot;http://delivery.acm.org/10.1145/980000/974190/p310-jing.pdf?ip=74.105.10.86&amp;amp;id=974190&amp;amp;acc=OPEN&amp;amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;amp;CFID=850146679&amp;amp;CFTOKEN=43433700&amp;amp;__acm__=1515685122_a0b141d0232617369741d8940165acb6&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Authors: Jey Han Lau, Alexander Clark, Shalom Lapin&lt;/li&gt;
  &lt;li&gt;Venue: Cognitive Science&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nutshell&quot;&gt;Nutshell&lt;/h3&gt;

&lt;p&gt;One of the things I really like about NLP is that it intersects with philosophy,
linguistics and cognitive science. This paper comes from that interesting middle
area. The authors use language models (from NLP) to predict which sentences are
acceptable and which sentences are not acceptable. They then use the accuracy of
such predictions to claim that “linguistic knowledge can be intrinsically probabilistic”.&lt;/p&gt;

&lt;p&gt;I want to mostly focus on what they authors did in the paper, then briefly summarize
their much deeper claims about the nature of human linguistic knowledge.&lt;/p&gt;

&lt;p&gt;In linguistics, an &lt;strong&gt;&lt;a href=&quot;http://www.socsci.uci.edu/~jsprouse/papers/Judgment%20data.pdf&quot;&gt;acceptability judgement&lt;/a&gt;&lt;/strong&gt; is a self-reported human description of the naturalness or wellformedness of a linguistic expression. Acceptability judgements can inform theories involving many, many areas of linguistics. How to solicit and interpret such judgements is a major topic within the field. In natural language processing, a &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;language model&lt;/a&gt;&lt;/strong&gt; assigns a probability to a sequence of words. The basic idea of this paper is that language models (from NLP) can be used to predict acceptability.&lt;/p&gt;</content><author><name></name></author><summary type="html">Title: Grammaticality, Acceptability and Probability PDF Authors: Jey Han Lau, Alexander Clark, Shalom Lapin Venue: Cognitive Science</summary></entry><entry><title type="html">Quasi-Synchronous Grammars</title><link href="http://localhost:4000/formalisms/mt/2018/01/15/quasi-synchronous_grammars.html" rel="alternate" type="text/html" title="Quasi-Synchronous Grammars" /><published>2018-01-15T12:40:53-05:00</published><updated>2018-01-15T12:40:53-05:00</updated><id>http://localhost:4000/formalisms/mt/2018/01/15/quasi-synchronous_grammars</id><content type="html" xml:base="http://localhost:4000/formalisms/mt/2018/01/15/quasi-synchronous_grammars.html">&lt;ul&gt;
  &lt;li&gt;Quasi-Synchronous Grammars: &lt;a href=&quot;https://www.cs.jhu.edu/~dasmith/qg_smt_2006.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Authors: David A. Smith and Jason Eisner&lt;/li&gt;
  &lt;li&gt;Venue: Workshop on Statistical Machine Translation, 2006&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;This paper from David A. Smith and Jason Eisner introduces a new formalism, a
‘‘Quasi-Synchronous Grammar” (QG), motivated by problems in machine translation. I found it via &lt;a href=&quot;http://www.aclweb.org/anthology/D11-1038&quot;&gt;Woodsend and Lapata (2010)&lt;/a&gt;, which applies the formalism to sentence simplification. I’ve also seen such grammars applied to &lt;a href=&quot;https://www.aclweb.org/anthology/D/D07/D07-1003.pdf&quot;&gt;question answering&lt;/a&gt;. Given that QGs seem to be a (somewhat obscure) part of the &lt;a href=&quot;http://repository.cmu.edu/cgi/viewcontent.cgi?article=1205&amp;amp;context=lti&quot;&gt;NLP research world&lt;/a&gt; and because I am sort of sick reading &lt;a href=&quot;/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html&quot;&gt;modifications&lt;/a&gt; of &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bandanau et al. 2014&lt;/a&gt; I decided to really dig into Smith and Eisner’s approach.&lt;/p&gt;

&lt;p&gt;At a high level, the paper does the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Introduce the QG formalism&lt;/li&gt;
  &lt;li&gt;Describe how to parameterize a probabilistic QG&lt;/li&gt;
  &lt;li&gt;Evaluate a QG on a machine translation task.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-formalism&quot;&gt;The formalism&lt;/h2&gt;

&lt;p&gt;QGs are an extension of an earlier formalism: the synchronous grammar.  &lt;script type=&quot;math/tex&quot;&gt;T_1&lt;/script&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Quasi-Synchronous Grammars: PDF Authors: David A. Smith and Jason Eisner Venue: Workshop on Statistical Machine Translation, 2006</summary></entry><entry><title type="html">Sentence Reduction for Automatic Text Summarization</title><link href="http://localhost:4000/summarization/compression/2018/01/11/sentence-redution-for-automatic-text-summarization.html" rel="alternate" type="text/html" title="Sentence Reduction for Automatic Text Summarization" /><published>2018-01-11T12:40:53-05:00</published><updated>2018-01-11T12:40:53-05:00</updated><id>http://localhost:4000/summarization/compression/2018/01/11/sentence-redution-for-automatic-text-summarization</id><content type="html" xml:base="http://localhost:4000/summarization/compression/2018/01/11/sentence-redution-for-automatic-text-summarization.html">&lt;ul&gt;
  &lt;li&gt;Title: Sentence Reduction for Automatic Text Summarization &lt;a href=&quot;http://delivery.acm.org/10.1145/980000/974190/p310-jing.pdf?ip=74.105.10.86&amp;amp;id=974190&amp;amp;acc=OPEN&amp;amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;amp;CFID=850146679&amp;amp;CFTOKEN=43433700&amp;amp;__acm__=1515685122_a0b141d0232617369741d8940165acb6&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Authors: Hongyan Jing&lt;/li&gt;
  &lt;li&gt;Venue: Applied Natural Language Processing Conference (2000)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nutshell&quot;&gt;Nutshell&lt;/h3&gt;

&lt;p&gt;This is a very early sentence compression paper from Hongyan Jing, a student of &lt;a href=&quot;http://www.cs.columbia.edu/~kathy/&quot;&gt;Kathleen McKeown&lt;/a&gt;. It was really interesting for me to read because I’ve looked at a bunch of recent work on the same topic.&lt;/p&gt;

&lt;p&gt;To shorten a sentence, Jing does the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;assigns an &lt;strong&gt;importance score&lt;/strong&gt; using WordNet to each phrase in the sentence.&lt;/li&gt;
  &lt;li&gt;uses a supervised corpus of summary–document pairs to determine the probability of pruning a constituent subtree of a given type, conditioned on its parent node.&lt;/li&gt;
  &lt;li&gt;uses a parser and lexical resource to identify mandatory grammatical components, guided by manual rules.&lt;/li&gt;
  &lt;li&gt;traverses the constituent parse tree from top to bottom, pruning subtrees which (1) are not grammaticality necessary (2) have a high probably of removal by a human annotator and (3) have low importance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jing evaluates her method using a custom metric, the ‘‘success rate’’, which measures the degree of overlap between a computer and a human annotator.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;One thing which is striking is the deep similarity between this work and later approaches to sentence compression. The core concerns in Jing (2000), &lt;a href=&quot;http://www.aclweb.org/anthology/W08-1105&quot;&gt;Filipova and Strube&lt;/a&gt; (2008) and &lt;a href=&quot;http://www.jair.org/papers/paper2433.html&quot;&gt;Clarke and Lapata&lt;/a&gt; (2010) are quite similar: the goal is to retain ‘‘important’’ portions of a source sentence while retaining grammaticality. The techniques for determining how to automatically meet these criteria have shifted, but in many ways the basic ideas are the same. Of course, &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43852.pdf&quot;&gt;recent&lt;/a&gt; sentence compression &lt;a href=&quot;/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html&quot;&gt;papers&lt;/a&gt; use neural networks to try and replicate these aims in a “data-driven” fashion.&lt;/p&gt;

&lt;p&gt;Based on 20 years of research, it seems like the sentence compression agenda in NLP might be &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41393.pdf&quot;&gt;more data&lt;/a&gt; + better modeling = closer automated replication of human summarization decisions. (As measured by ROUGE score or Pyramid or some other metric).&lt;/p&gt;

&lt;p&gt;This paradigm is common across NLP. But for summarization, the approach often annoys me: it seems obvious that different kinds of users will have different information needs. Query-focused summarization is supposed to address this issue, but I’m sort of dubious that there even exists a “gold standard”, &lt;em&gt;perfect&lt;/em&gt; summary for a given query. There seem to be deep limitations to this “annotate and model” paradigm, at least for sentence compression. Maybe a topic for a  &lt;a href=&quot;https://summarization2017.github.io/&quot;&gt;summarization&lt;/a&gt; workshop down the line.&lt;/p&gt;</content><author><name></name></author><summary type="html">Title: Sentence Reduction for Automatic Text Summarization PDF Authors: Hongyan Jing Venue: Applied Natural Language Processing Conference (2000)</summary></entry><entry><title type="html">Neural Summarization by Extracting Sentences and Words</title><link href="http://localhost:4000/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html" rel="alternate" type="text/html" title="Neural Summarization by Extracting Sentences and Words" /><published>2018-01-10T12:40:53-05:00</published><updated>2018-01-10T12:40:53-05:00</updated><id>http://localhost:4000/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words</id><content type="html" xml:base="http://localhost:4000/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html">&lt;ul&gt;
  &lt;li&gt;Title: Neural Summarization by Extracting Sentences and Words &lt;a href=&quot;https://arxiv.org/pdf/1603.07252.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Authors: Jianpeng Cheng and Mirella Lapata&lt;/li&gt;
  &lt;li&gt;Venue: ACL 2016&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;I found this paper on neural summarization while searching for neural network approaches
to sentence compression. Mirella Lapata has co-authored &lt;a href=&quot;http://www.jair.org/media/2433/live-2433-3731-jair.pdf&quot;&gt;influential papers&lt;/a&gt;
on compression using classical methods, so I was curious to read his deep learning approach.
The paper also uses &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;&lt;em&gt;pointer networks&lt;/em&gt;&lt;/a&gt;, which I have seen in
&lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;other recent&lt;/a&gt; summarization papers, another reason I chose to read this one in depth.&lt;/p&gt;

&lt;p&gt;According to Cheng and Lapata (sec 7), the main contributions of the paper are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;offering a hierarchical, neural model which creates a summary by choosing words and sentences, which they say reflects “the nature of the summarization task”.&lt;/li&gt;
  &lt;li&gt;generation by extraction, by which they mean generating output, using words from a single document (rather than the vocabulary of the whole corpus).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper is a nice synthesis and extension of much recent work on neural summarization using sequence-to-sequence models, such as &lt;a href=&quot;https://arxiv.org/pdf/1509.00685.pdf&quot;&gt;Rush, Chopra, Weston (2015)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;nutshell&quot;&gt;Nutshell&lt;/h3&gt;

&lt;p&gt;The authors collect gold standard summaries by downloading article/highlight summary summary pairs from the DailyMail, &lt;a href=&quot;https://arxiv.org/pdf/1606.02858.pdf&quot;&gt;a common dataset&lt;/a&gt;. They then use a series of hand-written rules to identify which sentences from an article ‘‘match a highlight’’. They also filter articles with highlights containing words which are &lt;em&gt;not&lt;/em&gt; drawn from a given document, as their model generates ‘‘by extraction’’.&lt;/p&gt;

&lt;p&gt;Using this DailyMail dataset for supervision, the authors present a somewhat complex, multi-component neural network model which:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;uses a convolutional neural network (CNN) to create a sentence embedding for each sentence in the input document&lt;/li&gt;
  &lt;li&gt;feeds the sequence of sentence embeddings into an LSTM to create a document embedding&lt;/li&gt;
  &lt;li&gt;decodes the document embedding timestep-by-timestep&lt;/li&gt;
  &lt;li&gt;uses the state of the decoder as input to a &lt;strong&gt;hierarchical&lt;/strong&gt; attention mechanism over first (a) each sentence in the document and then (b) each word type in the document to finally generate a word for the output sequence.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They then evaluate their model against competing approaches using ROUGE scores on the &lt;a href=&quot;http://duc.nist.gov/&quot;&gt;DUC 2002&lt;/a&gt; and DailyMail datasets.&lt;/p&gt;

&lt;p&gt;The authors argue that that step 4 is a hybrid of extractive and true abstractive summarization. In many respects, step 4 (section 4.3 in the paper) represents the major contribution of the work.&lt;/p&gt;

&lt;h3 id=&quot;details&quot;&gt;Details&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In section 4.3, the authors present a model which generates a word in a summary instead of a label for each sentence in the input sequence. Doesn’t this then imply that the summary must have as many word tokens as the number of sentences in the document? If so, this is a sort of strange constraint for a model. If the pitch for this approach is that it offers a model which reflects “the nature of the summarization task”, this seems like a significant limitation.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Title: Neural Summarization by Extracting Sentences and Words PDF Authors: Jianpeng Cheng and Mirella Lapata Venue: ACL 2016</summary></entry></feed>