<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-11T11:00:20-05:00</updated><id>http://localhost:4000/</id><title type="html">Abe Handler</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Neural Summarization by Extracting Sentences and Words</title><link href="http://localhost:4000/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html" rel="alternate" type="text/html" title="Neural Summarization by Extracting Sentences and Words" /><published>2018-01-10T12:40:53-05:00</published><updated>2018-01-10T12:40:53-05:00</updated><id>http://localhost:4000/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words</id><content type="html" xml:base="http://localhost:4000/summarization/neuralnets/2018/01/10/neural-summarization-by-extracting-sentences-and-words.html">&lt;ul&gt;
  &lt;li&gt;Title: Neural Summarization by Extracting Sentences and Words &lt;a href=&quot;https://arxiv.org/pdf/1603.07252.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Authors: Jianpeng Cheng and Mirella Lapata&lt;/li&gt;
  &lt;li&gt;Venue: ACL 2016&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;I found this paper on neural summarization while searching for neural network approaches
to sentence compression. Mirella Lapata has co-authored &lt;a href=&quot;http://www.jair.org/media/2433/live-2433-3731-jair.pdf&quot;&gt;influential papers&lt;/a&gt;
on compression using classical methods, so I was curious to read his deep learning approach.
The paper also uses &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;&lt;em&gt;pointer networks&lt;/em&gt;&lt;/a&gt;, which I have seen in
&lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;other recent&lt;/a&gt; summarization papers, another reason I chose to read this one in depth.&lt;/p&gt;

&lt;p&gt;According to Cheng and Lapata (sec 7), the main contributions of the paper are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;offering a hierarchical, neural model which creates a summary by choosing words and sentences, which they say reflects “the nature of the summarization task”.&lt;/li&gt;
  &lt;li&gt;generation by extraction, by which they mean generating output, using words from a single document (rather than the vocabulary of the whole corpus).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper is a nice synthesis and extension of much recent work on neural summarization using sequence-to-sequence models, such as &lt;a href=&quot;https://arxiv.org/pdf/1509.00685.pdf&quot;&gt;Rush, Chopra, Weston (2015)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;nutshell&quot;&gt;Nutshell&lt;/h3&gt;

&lt;p&gt;The authors collect gold standard summaries by downloading article/highlight summary summary pairs from the DailyMail, &lt;a href=&quot;https://arxiv.org/pdf/1606.02858.pdf&quot;&gt;a common dataset&lt;/a&gt;. They then use a series of hand-written rules to identify which sentences from an article ‘‘match a highlight’’. They also filter articles with highlights containing words which are &lt;em&gt;not&lt;/em&gt; drawn from a given document, as their model generates ‘‘by extraction’’.&lt;/p&gt;

&lt;p&gt;Using this DailyMail dataset for supervision, the authors present a somewhat complex, multi-component neural network model which:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;uses a convolutional neural network (CNN) to create a sentence embedding for each sentence in the input document&lt;/li&gt;
  &lt;li&gt;feeds the sequence of sentence embeddings into an LSTM to create a document embedding&lt;/li&gt;
  &lt;li&gt;decodes the document embedding timestep-by-timestep&lt;/li&gt;
  &lt;li&gt;uses the state of the decoder as input to a &lt;strong&gt;hierarchical&lt;/strong&gt; attention mechanism over first (a) each sentence in the document and then (b) each word type in the document to finally generate a word for the output sequence.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They then evaluate their model against competing approaches using ROUGE scores on the &lt;a href=&quot;http://duc.nist.gov/&quot;&gt;DUC 2002&lt;/a&gt; and DailyMail datasets.&lt;/p&gt;

&lt;p&gt;The authors argue that that step 4 is a hybrid of extractive and true abstractive summarization. In many respects, step 4 (section 4.3 in the paper) represents the major contribution of the work.&lt;/p&gt;

&lt;h3 id=&quot;details&quot;&gt;Details&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In section 4.3, the authors present a model which generates a word in a summary instead of a label for each sentence in the input sequence. Doesn’t this then imply that the summary must have as many word tokens as the number of sentences in the document? If so, this is a sort of strange constraint for a model. If the pitch for this approach is that it offers a model which reflects “the nature of the summarization task”, this seems like a significant limitation.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Title: Neural Summarization by Extracting Sentences and Words PDF Authors: Jianpeng Cheng and Mirella Lapata Venue: ACL 2016</summary></entry></feed>