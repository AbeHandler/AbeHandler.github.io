{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phrases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeHandler/AbeHandler.github.io/blob/master/Phrases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving from words to phrases when doing NLP\n",
        "- [Abe Handler](https://www.abehandler.com/) University of Colorado, Boulder\n",
        "- [Shufan Wang](https://people.cs.umass.edu/~shufanwang/) University of  Massachusetts, Amherst\n",
        "\n",
        "## Introduction\n",
        "\n",
        "If you have found this tutorial (companion slides [here](https://docs.google.com/presentation/d/1C5O0EdgM33SO1KlCk90rW1g7nuf2264XcuYPZB-b7bI/edit?usp=sharing)), you have probably done NLP projects where you (a) start with documents, (b) break them into individual words, and then (c) use computation to draw conclusions about the words in the documents. For instance, in your last project, maybe you took a collection of documents, broke the documents into individual words and then ran a topic model to find groups of words that tended to appear together in the documents. \n",
        "\n",
        "Breaking documents into individual words implicitly represents text in terms of single-word units called **unigrams**. The unordered collection of all the unigrams in a document is often called a   **bag of words** (see [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/4.pdf)). Representing text using the unigram bag of words has many advantages. For one, analyzing unigrams is easy and fast; you can take a big text document and break it into a bunch of single-word observations, so you can observe useful statistical properties from the text. \n",
        "\n",
        "However, breaking documents into single words for downstream analysis does have downsides. One limitation is that some concepts or linguistic units within documents consist of multiple words, and so get lost or discarded when you using a unigram representation. For instance, the string \"immigration hearing\" refers to a particular legal proceeding. If you break this string into single word units \"immigration\" and \"hearing\", and put each of these unigrams to your bag of words, your representation of the text does not really represent the concept \"immigration hearing\".  That means when you draw conclusions about lexical units during downsteam analysis, you won't be able to draw conclusions about \"immigration hearing\" (e.g., what liberal and conservative judges say about an \"immigration hearing\"). \n",
        "\n",
        "For this reason, it sometimes may make sense for you to analyze groups of words instead of unigrams. This tutorial will show you how to do NLP with groups of words, which we will call **phrases** or **multi-word expressions**. We will show how to (A) extract phrases from documents and (B) use these phrases for downstream analysis. \n",
        "\n",
        "## High-level takeaways\n",
        "\n",
        "1. **You can use phrases when you do NLP.** There are many existing tools and methods for extracting phrases (e.g. [PyATE](https://github.com/kevinlu1248/pyate)). In this tutorial, we will explore using phrases extracted via Python package [phrasemachine](https://github.com/slanglab/phrasemachine) which you can install using `pip install phrasemachine`. Phrasemachine is based on the method described in the paper [Bag of What](https://aclanthology.org/W16-5615.pdf).\n",
        "2. **You can be creative and define phrases in a way that makes sense for your problem.**  `phrasemachine` uses a grammar over part-of-speech tags to extract phrases. The particular phrasemachine patterns are often useful. But in your work, think of other kinds of phrasal patterns you might want to extract using regular expressions. For instance, if you are analyzing the political valance of economic theories, you might want to search economics papers for a pattern like \"theory of \\$ADJ\\?(NOUN|PROPN)+\" (e.g. \"Theory of Monetary Policy\").\n",
        "3. **Phrasemachine extracts discrete phrases, which comes with downsides and limitations**. Extracted discrete phrases are not distributed; so similar phrases get totally different discrete representations. Also, extracted discrete phrases are not contextual, meaning that representations do not reflect the different meaning of phrases in context. (An \"immigration hearing\" is different if it is an \"EU immigration hearing\" or an \"immigration hearing in Texas\"). Phrase-BERT (coming up!) addresses these real issues with phrasemachine, but does come with greater computational costs and engineering burdens. "
      ],
      "metadata": {
        "id": "d6wxORogR755"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/cases.jsonl -O cases.jsonl\n",
        "! pip install phrasemachine\n",
        "! pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_GLnbRwemk6",
        "outputId": "ba102ce9-22b1-432d-efda-cbfd15fe393c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-15 18:55:41--  https://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/cases.jsonl\n",
            "Resolving zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)... 128.253.51.178\n",
            "Connecting to zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)|128.253.51.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13337468 (13M) [application/octet-stream]\n",
            "Saving to: ‘cases.jsonl’\n",
            "\n",
            "cases.jsonl         100%[===================>]  12.72M  11.7MB/s    in 1.1s    \n",
            "\n",
            "2022-02-15 18:55:42 (11.7 MB/s) - ‘cases.jsonl’ saved [13337468/13337468]\n",
            "\n",
            "Requirement already satisfied: phrasemachine in /usr/local/lib/python3.7/dist-packages (1.0.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from phrasemachine) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->phrasemachine) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->phrasemachine) (2022.1.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->phrasemachine) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->phrasemachine) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install convokit==2.5.2 # later versions seem to have some issue w/ a torch dependency. If you go to runtime-> restart runtime this seems to work (AH: Feb 15, 22)"
      ],
      "metadata": {
        "id": "27rCxSbClvm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from convokit import Corpus, download # install the corpus here\n",
        "corpus = Corpus(filename=download(\"supreme-corpus\")) # download the corpus"
      ],
      "metadata": {
        "id": "N5PLWwu-kwpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unigram bag of words\n",
        "\n",
        "Let's start with a single (short) document and break it into a unigram bag of words. It's easy to find packages for this online, but we will just use vanilla Python for this. A few notes:\n",
        "\n",
        "- We will define words using a whitespace delimiter below, but note there are also other better ways to do [tokenization](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf).\n",
        "- Note that a [bag](https://en.wikipedia.org/wiki/Multiset) is a set that allows duplicates; notice that the word `a` appears two times in the `unigram_bag_of_words`.\n",
        "- Note that each item in our bag is a unigram (single word)"
      ],
      "metadata": {
        "id": "bjwKfeJqpHSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "document =  \"Solyndra received a loan guarantee. The Department of Energy offered the guarantee.\".replace(\".\",\"\")\n",
        "unigram_bag_of_words = defaultdict(int)\n",
        "for word in document.split():\n",
        "    unigram_bag_of_words[word] += 1\n",
        "\n",
        "unigram_bag_of_words"
      ],
      "metadata": {
        "id": "OkXB1_A6pCjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion: what are some phrases that get missed?"
      ],
      "metadata": {
        "id": "21N8xzxhsTI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding phrases to the bag of words"
      ],
      "metadata": {
        "id": "XcQWcPG7sjLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import phrasemachine\n",
        "from tqdm.notebook import tqdm\n",
        "text = \"Solyndra received a loan guarantee. The Department of Energy offered the guarantee.\".replace(\".\",\"\")\n",
        "out = phrasemachine.get_phrases(text)\n",
        "out"
      ],
      "metadata": {
        "id": "gw3KSip_To3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we are adding the phrases to the unigram bag of words\n",
        "\n",
        "enriched_bag_of_words = unigram_bag_of_words\n",
        "for phrase in out[\"counts\"]:\n",
        "    enriched_bag_of_words[phrase] = out['counts'][phrase]\n",
        "\n",
        "enriched_bag_of_words"
      ],
      "metadata": {
        "id": "kYbHS26Wvn9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using phrases for downstream analysis\n",
        "\n",
        "Now that we know how to extract phrases using phrasemachine, we will now see how to use such phrases for downstream analysis. Specifically, we will analyze the ideological orientation of words and phrases in U.S. Supreme Court Oral Arguments.\n",
        "\n",
        "At a high level, we will ask: what kinds of things to liberal and conservative justices tend to bring up during oral arguments? What would you expect to liberals and convservatives to talk about?"
      ],
      "metadata": {
        "id": "6ccOnhLchBiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Corpus\n",
        "We will use the [`convokit`](https://convokit.cornell.edu/documentation/supreme.html) corpus of supreme court oral arguments. In this notebook, we will only examine comments from liberal and conservative justices from the years 2010-2019."
      ],
      "metadata": {
        "id": "v0qmiSKb0vpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis, we will investigate which phrases are used by liberal (L) and conservative (C) justices. So we need a mapping of justices to ideologies, which we construct manually below."
      ],
      "metadata": {
        "id": "p8fnkx_Yh8cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "judge2ideology = {'j__john_g_roberts_jr': \"C\", \n",
        "                  'j__samuel_a_alito_jr': \"C\",\n",
        "                  'j__ruth_bader_ginsburg': \"L\",\n",
        "                  'j__sonia_sotomayor': \"L\",\n",
        "                  'j__antonin_scalia': \"L\",\n",
        "                  'j__stephen_g_breyer': \"L\",\n",
        "                  'j__anthony_m_kennedy': \"C\",\n",
        "                  'j__elena_kagan': \"L\",\n",
        "                  'j__clarence_thomas': \"C\",\n",
        "                  'j__neil_gorsuch': \"C\",\n",
        "                  'j__brett_m_kavanaugh': \"C\"\n",
        "                  }"
      ],
      "metadata": {
        "id": "gxREkwzx4t7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also build a set of all justices in the dataset."
      ],
      "metadata": {
        "id": "L49_rVmXit9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def get_justices(input_file=\"cases.jsonl\"):\n",
        "    '''Get names of all justices in the dataset'''\n",
        "    all_justices = set()\n",
        "    with open(input_file, \"r\") as inf:\n",
        "        for j in inf:\n",
        "            j = json.loads(j)\n",
        "            if j[\"votes\"] is not None:\n",
        "                for justice in j[\"votes\"].keys():\n",
        "                    all_justices.add(justice)\n",
        "    return all_justices\n",
        "\n",
        "all_justices = get_justices()"
      ],
      "metadata": {
        "id": "U-gYS_NRis0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to extract words and phrases for the justices. In computing this information, we do two things to make the computational requirements more managable: \n",
        "1. We limit our analysis to court cases from 2010-2019, which is why you see `u.meta[\"case_id\"][0:3] == \"201\"` below. \n",
        "2. We also only extract phrases from the first 500 characters of the utterance."
      ],
      "metadata": {
        "id": "5bJ_Hjali8lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utterances = [] # build a list of the utterances we are interested in\n",
        "for u in tqdm(corpus.get_utterance_ids()):\n",
        "    u = corpus.get_utterance(u)\n",
        "    if u.speaker.id in all_justices and u.meta[\"case_id\"][0:3] == \"201\":\n",
        "        utterances.append(u)"
      ],
      "metadata": {
        "id": "IGPoa4e0mEbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting phrases\n",
        "\n",
        "The code below extracts and counts phrases from liberal and conservative justices"
      ],
      "metadata": {
        "id": "x5OWNu952PZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict # https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "justice2phrases = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for u in tqdm(utterances):\n",
        "    phrases = phrasemachine.get_phrases(u.text[0:500])[\"counts\"] # roughly 97.5% are less than 500 chars, and runs way faster\n",
        "    for p in phrases:\n",
        "        # we can filter out some filler/stop phrases here, e.g. when the record notes laughter\n",
        "        if \"justice\" not in p and \"mr.\" not in p and \"minutes\" not in p and \"laugher\" not in p:\n",
        "            justice2phrases[judge2ideology[u.speaker.id]][p] += phrases[p]\n",
        "\n",
        "phrasecounts = justice2phrases # this builds a dictionary of count of phrases by liberal/conservative judges"
      ],
      "metadata": {
        "id": "2ZFIyJ5TOjMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting words\n",
        "\n",
        "The code below extracts and counts unigrams from liberal and conservative justices"
      ],
      "metadata": {
        "id": "-PVVQVjr2X5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "justice2words = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for u in tqdm(utterances):\n",
        "    words = u.text[0:500].split()\n",
        "    for word in words:\n",
        "        # we can filter out some filler/stop phrases here, e.g. when the record notes laughter\n",
        "        if \"justice\" not in word and \"mr.\" not in word and \"minutes\" not in word and \"laugher\" not in word:\n",
        "            justice2words[judge2ideology[u.speaker.id]][word] += 1\n",
        "\n",
        "wordcounts = justice2words # this builds a dictionary of count of phrases by liberal/conservative judges"
      ],
      "metadata": {
        "id": "I0FrnAkOv3GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing word use\n",
        "\n",
        "Now that we have counted words and phrases from liberal and convservative justices, we will analyze differences in the political orientation of words and phrases. Specifically we will:\n",
        "- Compute statistics about how frequently liberal and conservative justices use particular words and \n",
        "- Display this information on a plot for analysis \n",
        "\n",
        "Our approach is based on the [Fightin' Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) method from Monroe et al. Specifically, we will use the word importance score from Section 3.2.2 of Fightin' Words. If you are curious, the paper describes other word importance scores."
      ],
      "metadata": {
        "id": "u8V4jCA41_xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_normalize_counts(_countdict):\n",
        "\n",
        "    normalized_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for wing in _countdict.keys():\n",
        "        for p in _countdict[wing]:\n",
        "            normalized_counts[wing][p] = _countdict[wing][p]/n[wing]\n",
        "\n",
        "    return normalized_counts\n",
        "\n",
        "def compute_phrase_scores(normalized, _countdict):\n",
        "    df = []\n",
        "    for wing in _countdict.keys():\n",
        "        for phrase in _countdict[wing]:\n",
        "            df.append({\"score\": normalized[\"L\"][phrase] - normalized[\"C\"][phrase], \n",
        "                    \"phrase\": phrase,\n",
        "                    \"count\": _countdict[\"C\"][phrase] + _countdict[\"L\"][phrase]}) # http://languagelog.ldc.upenn.edu/myl/Monroe.pdf, 3.2.2\n",
        "    return pd.DataFrame(df).drop_duplicates()\n",
        "\n",
        "def getK(_df, k=20):\n",
        "    if k > 0:\n",
        "        return _df.sort_values(\"score\")[0:k].copy()\n",
        "    else:\n",
        "        return _df.sort_values(\"score\")[k:].copy()\n",
        "\n",
        "def get_top_K_df(counts):\n",
        "\n",
        "    countdict = counts\n",
        "\n",
        "    n = {}  \n",
        "    n[\"C\"] = sum(countdict[\"C\"].values())\n",
        "    n[\"L\"] = sum(countdict[\"L\"].values())\n",
        "\n",
        "    normalized_counts = compute_normalize_counts(countdict)\n",
        "\n",
        "    df = compute_phrase_scores(normalized_counts, countdict)\n",
        "\n",
        "    df = df[df[\"count\"] < 200] # exclude high-count lexical items, roughly stop words\n",
        "\n",
        "    # add a label field to the data frame for altair\n",
        "    df[\"label\"] = df[\"phrase\"].apply(lambda x: x if x in tops[\"phrase\"].to_list() else \"\")\n",
        "\n",
        "    # add an abolute value of the score\n",
        "    df[\"score_abs\"] = df[\"score\"].apply(lambda x: abs(x))\n",
        "\n",
        "    return pd.concat([getK(df, k=-20), getK(df, k=20)])\n",
        "\n",
        "\n",
        "tops_phrases = get_top_K_df(phrasecounts)\n",
        "\n",
        "tops_words = get_top_K_df(wordcounts)\n",
        "\n",
        "tops_words[\"label\"] = tops_words[\"phrase\"]"
      ],
      "metadata": {
        "id": "j5xg7QcUuKno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add words to doc?"
      ],
      "metadata": {
        "id": "JQ7ODlDVvp2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def make_plot(source):\n",
        "\n",
        "    height = 1000\n",
        "\n",
        "    points = alt.Chart(source).mark_circle().encode(\n",
        "        x='count:Q',\n",
        "        y='score:Q',\n",
        "        size='score_abs',\n",
        "        color=alt.Color('score:Q', scale=alt.Scale(scheme='redyellowblue'))\n",
        "    ).properties(\n",
        "        width=1200,\n",
        "        height=height\n",
        "    )\n",
        "\n",
        "    text = alt.Chart(source).mark_text(\n",
        "        align='left',\n",
        "        baseline='middle',\n",
        "        dx=7\n",
        "    ).encode(\n",
        "        x='count:Q',\n",
        "        y='score:Q',\n",
        "        text='label'\n",
        "    ).properties(\n",
        "        width=1200,\n",
        "        height=height\n",
        "    )\n",
        "\n",
        "    return points + text\n",
        "\n",
        "make_plot(tops_phrases)"
      ],
      "metadata": {
        "id": "tgfVnKVaMxKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_plot(tops_words)"
      ],
      "metadata": {
        "id": "G6jqvWwQ7ZEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion\n",
        "\n",
        "- Comparing the unigram plot to the plot with phrases, what do you notice? \n",
        "\n",
        "- Which plot gives you a clearer sense what justices tend to talk about. This is sometimes called being more \"[interpretable](https://arxiv.org/abs/1702.08608)\"."
      ],
      "metadata": {
        "id": "wkj3u_lj8i_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import phrasemachine\n",
        "\n",
        "utterances = [] # build a list of the utterances we are interested in\n",
        "c = 0 \n",
        "\n",
        "def theory_phrases(_text):\n",
        "    for phrase in phrasemachine.get_phrases(_text)[\"counts\"]:\n",
        "        if phrase[0:len(\"theory of\")] == \"theory of\":\n",
        "            yield phrase\n",
        "\n",
        "for u in tqdm(corpus.get_utterance_ids()):\n",
        "    u = corpus.get_utterance(u)\n",
        "    c += 1\n",
        "    if c == 100000:\n",
        "        break\n",
        "    if \"theory of\" in u.text:\n",
        "        for phrase in theory_phrases(u.text):\n",
        "            print(phrase)"
      ],
      "metadata": {
        "id": "90Vj3GMJ8tat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co"
      ],
      "metadata": {
        "id": "KMVoTPZSnHoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sApwmf0LoO2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}