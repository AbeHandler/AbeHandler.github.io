{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKHJY79SEEow1bdOXhXfnn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeHandler/AbeHandler.github.io/blob/master/runeconberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "from google.colab import userdata\n",
        "hf = userdata.get('huggingface_token')\n",
        "\n",
        "pipe = pipeline(\"token-classification\", model=\"abehandlerorg/econberta\", token=hf)\n",
        "\n",
        "# Function to preprocess the tagged token's word for matching\n",
        "def preprocess_word(word):\n",
        "    return word.replace('▁', '').lower()\n",
        "\n",
        "# Fuzzy matching function\n",
        "def is_match(spacy_token, tagged_token):\n",
        "    # Preprocess the tagged token's word\n",
        "    tagged_word = preprocess_word(tagged_token['word'])\n",
        "    # Check if the texts are similar enough (using a simple lowercase comparison here, but you can use fuzzier methods)\n",
        "    if spacy_token['text'].lower() == tagged_word:\n",
        "        # Check if the character offsets are similar\n",
        "        if abs(spacy_token['start'] - tagged_token['start']) <= 1 and abs(spacy_token['end'] - tagged_token['end']) <= 1:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "eOoTRiydHBMK"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzinYfVPnD1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "para = '''Using the size of CEO signatures in SEC filings to measure individual narcissism, we find that CEO narcissism is associated with several negative firm outcomes. We first validate signature size as a measure of narcissism but not overconfidence using two laboratory studies, and also find that our measure is correlated with employee perceptions of CEO narcissism used in prior research. We then use CEO signatures to study the relation between CEO narcissism and the firm’s investment policies and performance. CEO narcissism is associated with overinvestment, particularly in R&D and M&A expenditures (but not in capital expenditures). Firms led by narcissistic CEOs experience lower financial productivity in the form of profitability and operating cash flows. Despite this negative performance, narcissistic CEOs enjoy higher absolute and relative compensation. Our results are robust to several alternative specifications, including controlling for a popular options-based overconfidence measure used in prior research.'''\n",
        "taggedtokens = pipe(para)\n",
        "\n",
        "doc = nlp(para)\n",
        "tokens_list = []\n",
        "for i, token in enumerate(doc):\n",
        "    # Determine if there's whitespace after the token\n",
        "    # This checks if the next char in the original text is not the start of the next token\n",
        "    ws = False\n",
        "    if i + 1 < len(doc) and doc[i + 1].idx > token.idx + len(token):\n",
        "        ws = True\n",
        "\n",
        "    token_info = {\n",
        "        \"text\": token.text,\n",
        "        \"start\": token.idx,\n",
        "        \"end\": token.idx + len(token),\n",
        "        \"id\": i,\n",
        "        \"ws\": ws\n",
        "    }\n",
        "    tokens_list.append(token_info)\n",
        "\n",
        "for spacy_token in tokens_list:\n",
        "    for tagged_token in taggedtokens:\n",
        "        if is_match(spacy_token, tagged_token):\n",
        "            # Assign the entity label from the tagged token to the spaCy token\n",
        "            spacy_token['entity'] = tagged_token['entity'].replace(\"B-\", \"\").replace(\"I-\", \"\").upper()\n",
        "            break  # Stop searching after the first match for each spaCy token\n"
      ],
      "metadata": {
        "id": "Y8N9XhPWGJGo"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spans = []\n",
        "current_span = None\n",
        "\n",
        "for token in tokens_list:\n",
        "    # Check if the token has an entity label\n",
        "    if 'entity' in token:\n",
        "        if current_span is None:\n",
        "            # Start a new span\n",
        "            current_span = {\n",
        "                'start': token['start'],\n",
        "                'end': token['end'],\n",
        "                'token_start': token['id'],\n",
        "                'token_end': token['id'],\n",
        "                'label': token['entity']\n",
        "            }\n",
        "        else:\n",
        "            # Check if the current token continues the current span\n",
        "            if token['entity'] == current_span['label']:\n",
        "                # Update the end of the current span\n",
        "                current_span['end'] = token['end']\n",
        "                current_span['token_end'] = token['id']\n",
        "            else:\n",
        "                # The current token starts a new span, so finish the current span and start a new one\n",
        "                spans.append(current_span)\n",
        "                current_span = {\n",
        "                    'start': token['start'],\n",
        "                    'end': token['end'],\n",
        "                    'token_start': token['id'],\n",
        "                    'token_end': token['id'],\n",
        "                    'label': token['entity']\n",
        "                }\n",
        "    else:\n",
        "        # The current token does not have a label, so finish the current span (if any) and reset\n",
        "        if current_span is not None:\n",
        "            spans.append(current_span)\n",
        "            current_span = None\n",
        "\n",
        "# Add the last span if it hasn't been added yet\n",
        "if current_span is not None:\n",
        "    spans.append(current_span)\n",
        "\n",
        "out = {\"text\": para, \"tokens\": tokens_list, \"spans\": spans}"
      ],
      "metadata": {
        "id": "h7tIQBtR_bK_"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"annotations2.jsonl\", \"w\") as of:\n",
        "    of.write(json.dumps(out))"
      ],
      "metadata": {
        "id": "Zm72rZzGJFpr"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-gdID_KDZL",
        "outputId": "6fe3b674-6528-48bf-8f21-739b1a620564"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'start': 10,\n",
              "  'end': 32,\n",
              "  'token_start': 2,\n",
              "  'token_end': 5,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 70,\n",
              "  'end': 80,\n",
              "  'token_start': 12,\n",
              "  'token_end': 12,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 95,\n",
              "  'end': 98,\n",
              "  'token_start': 17,\n",
              "  'token_end': 17,\n",
              "  'label': 'POPULATION'},\n",
              " {'start': 99,\n",
              "  'end': 109,\n",
              "  'token_start': 18,\n",
              "  'token_end': 18,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 146,\n",
              "  'end': 159,\n",
              "  'token_start': 24,\n",
              "  'token_end': 25,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 179,\n",
              "  'end': 193,\n",
              "  'token_start': 30,\n",
              "  'token_end': 31,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 210,\n",
              "  'end': 220,\n",
              "  'token_start': 36,\n",
              "  'token_end': 36,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 352,\n",
              "  'end': 362,\n",
              "  'token_start': 58,\n",
              "  'token_end': 58,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 399,\n",
              "  'end': 402,\n",
              "  'token_start': 67,\n",
              "  'token_end': 67,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 444,\n",
              "  'end': 447,\n",
              "  'token_start': 74,\n",
              "  'token_end': 74,\n",
              "  'label': 'POPULATION'},\n",
              " {'start': 448,\n",
              "  'end': 458,\n",
              "  'token_start': 75,\n",
              "  'token_end': 75,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 474,\n",
              "  'end': 493,\n",
              "  'token_start': 80,\n",
              "  'token_end': 81,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 498,\n",
              "  'end': 509,\n",
              "  'token_start': 83,\n",
              "  'token_end': 83,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 511,\n",
              "  'end': 514,\n",
              "  'token_start': 85,\n",
              "  'token_end': 85,\n",
              "  'label': 'POPULATION'},\n",
              " {'start': 515,\n",
              "  'end': 525,\n",
              "  'token_start': 86,\n",
              "  'token_end': 86,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 589,\n",
              "  'end': 601,\n",
              "  'token_start': 97,\n",
              "  'token_end': 97,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 614,\n",
              "  'end': 634,\n",
              "  'token_start': 102,\n",
              "  'token_end': 103,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 637,\n",
              "  'end': 642,\n",
              "  'token_start': 106,\n",
              "  'token_end': 106,\n",
              "  'label': 'POPULATION'},\n",
              " {'start': 650,\n",
              "  'end': 662,\n",
              "  'token_start': 109,\n",
              "  'token_end': 109,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 663,\n",
              "  'end': 667,\n",
              "  'token_start': 110,\n",
              "  'token_end': 110,\n",
              "  'label': 'POPULATION'},\n",
              " {'start': 685,\n",
              "  'end': 707,\n",
              "  'token_start': 113,\n",
              "  'token_end': 114,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 723,\n",
              "  'end': 736,\n",
              "  'token_start': 119,\n",
              "  'token_end': 119,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 741,\n",
              "  'end': 761,\n",
              "  'token_start': 121,\n",
              "  'token_end': 123,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 785,\n",
              "  'end': 796,\n",
              "  'token_start': 128,\n",
              "  'token_end': 128,\n",
              "  'label': 'OUTCOME'},\n",
              " {'start': 798,\n",
              "  'end': 810,\n",
              "  'token_start': 130,\n",
              "  'token_end': 130,\n",
              "  'label': 'INTERVENTION'},\n",
              " {'start': 811,\n",
              "  'end': 815,\n",
              "  'token_start': 131,\n",
              "  'token_end': 131,\n",
              "  'label': 'POPULATION'},\n",
              " {'start': 829,\n",
              "  'end': 863,\n",
              "  'token_start': 134,\n",
              "  'token_end': 137,\n",
              "  'label': 'OUTCOME'}]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3o0ivBihKK_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}